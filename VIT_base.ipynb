{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNm3HgmgH7MWV3WOZGak/CD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wj-arit/VIT_implement/blob/main/VIT_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynkP0jZzemhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94af9eb6-22fd-43ba-f526-17e99f9005dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module, ModuleList"
      ],
      "metadata": {
        "id": "F4o1owlBijUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e69409f-fe32-4fc1-8401-270a1eca3fec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t,tuple) else(t,t)"
      ],
      "metadata": {
        "id": "CUVSOtYtg-O5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(Module):\n",
        "    def __init__(self,dim,hidden_dim,dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim,hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim,dim),\n",
        "            nn.Dropout(dropout)\n",
        "    )\n",
        "    def forward(self,x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "zH7phTipifUB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(Module):\n",
        "    def __init__(self,dim,heads=8,dim_head=64,dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not(heads==1 and dim_head == dim)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim,inner_dim*3,bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim,dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self,x):\n",
        "        self.x = self.norm(x)\n",
        "        qkv = self.to_qkv(x).chunk(3,dim=-1)\n",
        "        q,k,v = map(lambda t:rearrange(t,'b n (h d) -> b h n d',h=self.heads),qkv)\n",
        "\n",
        "        dots = torch.matmul(q,k.transpose(-1,-2))*self.scale\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn,v)\n",
        "        out = rearrange(out,'b h n d -> b n (h d)',h=self.heads)\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "bpIx-_2Yna_s"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(Module):\n",
        "    def __init__(self,dim,depth,heads,dim_head,mlp_dim,dropout=0.):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.layers = ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(ModuleList([\n",
        "                Attention(dim,heads=heads,dim_head=dim_head,dropout=dropout),\n",
        "                FeedForward(dim,mlp_dim,dropout=dropout)\n",
        "        ]))\n",
        "    def forward(self,x):\n",
        "        for attn,ff in self.layers:\n",
        "          x = attn(x) + x\n",
        "          x = ff(x) + x\n",
        "        return self.norm(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "wHj7ld530jm8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = channels * patch_height * patch_width\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        num_cls_tokens = 1 if pool == 'cls' else 0\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(num_cls_tokens, dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(num_patches + num_cls_tokens, dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Linear(dim, num_classes) if num_classes > 0 else None\n",
        "\n",
        "    def forward(self, img):\n",
        "        batch = img.shape[0]\n",
        "        x = self.to_patch_embedding(img)\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '... d -> b ... d', b = batch)\n",
        "        x = torch.cat((cls_tokens, x), dim = 1)\n",
        "\n",
        "        seq = x.shape[1]\n",
        "\n",
        "        x = x + self.pos_embedding[:seq]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        if self.mlp_head is None:\n",
        "            return x\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "27i_Htee4Bz5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_TAR = \"/content/drive/MyDrive/data/imagenet100.tar.gz\"\n",
        "WORK_DIR = \"/content/data\"\n",
        "IMAGENET_DIR = f\"{WORK_DIR}\"\n"
      ],
      "metadata": {
        "id": "SpRt_Fd39E0c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TAR_PATH = \"/content/drive/MyDrive/data/imagenet100.tar.gz\"\n",
        "\n",
        "print(\"exists:\", os.path.exists(TAR_PATH))\n",
        "print(\"size (MB):\", os.path.getsize(TAR_PATH) / 1024 / 1024 if os.path.exists(TAR_PATH) else None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpEvs9ewGCTL",
        "outputId": "e7a5934b-a1e2-46bd-aeb5-a5249f005dcd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists: True\n",
            "size (MB): 16476.764285087585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(\"/content/data\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohfdmc73GInB",
        "outputId": "8ce8b202-f59c-4cde-b914-aaac4897c09e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "# ===== 경로 설정 =====\n",
        "DRIVE_TAR = \"/content/drive/MyDrive/data/imagenet100.tar.gz\"\n",
        "WORK_DIR = \"/content/data\"\n",
        "\n",
        "# ===== 1) 작업 디렉토리 초기화 =====\n",
        "if os.path.exists(WORK_DIR):\n",
        "    shutil.rmtree(WORK_DIR)\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "print(\"WORK_DIR created:\", WORK_DIR)\n",
        "print(\"TAR exists:\", os.path.exists(DRIVE_TAR))\n",
        "\n",
        "# ===== 2) 압축 해제 =====\n",
        "print(\"Extracting ImageNet100...\")\n",
        "with tarfile.open(DRIVE_TAR, \"r:*\") as tar:\n",
        "    tar.extractall(WORK_DIR)\n",
        "print(\"Extraction done.\")\n",
        "\n",
        "# ===== 3) train / val 자동 탐색 =====\n",
        "IMAGENET_DIR = None\n",
        "for root, dirs, files in os.walk(WORK_DIR):\n",
        "    if \"train\" in dirs and \"val\" in dirs:\n",
        "        IMAGENET_DIR = root\n",
        "        break\n",
        "\n",
        "assert IMAGENET_DIR is not None, \"train/val 폴더를 찾지 못함\"\n",
        "\n",
        "# ===== 4) 결과 확인 =====\n",
        "print(\"\\n ImageNet root found:\")\n",
        "print(\"IMAGENET_DIR =\", IMAGENET_DIR)\n",
        "print(\"Contents:\", os.listdir(IMAGENET_DIR))\n",
        "print(\"train classes:\", len(os.listdir(os.path.join(IMAGENET_DIR, \"train\"))))\n",
        "print(\"val classes:\", len(os.listdir(os.path.join(IMAGENET_DIR, \"val\"))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APN3x7-tAuIT",
        "outputId": "195f307e-e86d-4496-da3c-62f2773d688f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORK_DIR created: /content/data\n",
            "TAR exists: True\n",
            "Extracting ImageNet100...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-305428251.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(WORK_DIR)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction done.\n",
            "\n",
            " ImageNet root found:\n",
            "IMAGENET_DIR = /content/data\n",
            "Contents: ['val', 'train']\n",
            "train classes: 100\n",
            "val classes: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "cd8oUddZB6h4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myCvXnGwB7T3",
        "outputId": "9ecd471c-5b3a-412d-a57d-2d962f1b0277"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 4\n"
      ],
      "metadata": {
        "id": "FamTE4EeB90k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.08, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225),\n",
        "    ),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225),\n",
        "    ),\n",
        "])\n"
      ],
      "metadata": {
        "id": "1dGa1ok4CBBU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"/content/data\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(DATA_ROOT, \"train\"),\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    root=os.path.join(DATA_ROOT, \"val\"),\n",
        "    transform=val_transform\n",
        ")\n"
      ],
      "metadata": {
        "id": "H_32xeb7EFZo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "Nh0QI0zNEGSs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "from einops import repeat\n",
        "\n",
        "model = ViT(\n",
        "    image_size=224,\n",
        "    patch_size=16,\n",
        "    num_classes=100,\n",
        "    dim=768,\n",
        "    depth=12,\n",
        "    heads=12,\n",
        "    mlp_dim=3072,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    weight_decay=0.05\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=100\n",
        ")\n"
      ],
      "metadata": {
        "id": "f3aA3ZF8EKUd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_DIR = \"/content/drive/MyDrive/vit_checkpoints\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "_Ti0l2qJESdF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(epoch, model, optimizer, scheduler, best_acc):\n",
        "    path = os.path.join(CKPT_DIR, f\"ckpt_epoch_{epoch}.pt\")\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"scheduler\": scheduler.state_dict(),\n",
        "        \"best_acc\": best_acc\n",
        "    }, path)\n"
      ],
      "metadata": {
        "id": "v3Q5qsCUES_b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_latest_checkpoint(model, optimizer, scheduler):\n",
        "    ckpts = glob.glob(os.path.join(CKPT_DIR, \"ckpt_epoch_*.pt\"))\n",
        "    if len(ckpts) == 0:\n",
        "        return 0, 0.0\n",
        "\n",
        "    ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "    ckpt_path = ckpts[-1]\n",
        "\n",
        "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(checkpoint[\"model\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
        "\n",
        "    print(f\"Resumed from {ckpt_path}\")\n",
        "    return checkpoint[\"epoch\"] + 1, checkpoint[\"best_acc\"]\n"
      ],
      "metadata": {
        "id": "EyGIWgSAEVdD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n"
      ],
      "metadata": {
        "id": "MdsasDdxEX70"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n"
      ],
      "metadata": {
        "id": "6Axx0z-IEb54"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "LOG_DIR = \"/content/drive/MyDrive/vit_logs\"\n",
        "writer = SummaryWriter(log_dir=LOG_DIR)\n"
      ],
      "metadata": {
        "id": "REz3bIiXE-CK"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "start_epoch, best_acc = load_latest_checkpoint(\n",
        "    model, optimizer, scheduler\n",
        ")\n",
        "\n",
        "for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # ---- train / val ----\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, optimizer\n",
        "    )\n",
        "    val_loss, val_acc = validate(model, val_loader)\n",
        "\n",
        "    # ---- lr schedule ----\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    # ---- best acc update ----\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "\n",
        "    # ---- checkpoint ----\n",
        "    save_checkpoint(epoch, model, optimizer, scheduler, best_acc)\n",
        "\n",
        "    # ---- TensorBoard logging ----\n",
        "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "\n",
        "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
        "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
        "    writer.add_scalar(\"Accuracy/best_val\", best_acc, epoch)\n",
        "\n",
        "    writer.add_scalar(\"LR\", current_lr, epoch)\n",
        "\n",
        "    # ---- console log ----\n",
        "    print(\n",
        "        f\"[Epoch {epoch}] \"\n",
        "        f\"Train loss {train_loss:.4f} | acc {train_acc:.4f} || \"\n",
        "        f\"Val loss {val_loss:.4f} | acc {val_acc:.4f} || \"\n",
        "        f\"Best acc {best_acc:.4f} || \"\n",
        "        f\"LR {current_lr:.6f} || \"\n",
        "        f\"Time {time.time() - t0:.1f}s\"\n",
        "    )\n",
        "writer.close()\n"
      ],
      "metadata": {
        "id": "oNUHb01kEivP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/vit_logs\n"
      ],
      "metadata": {
        "id": "Hn9LtdHIFC3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}